---
title: Resources
nav: true
---

# Resources

National web archives:

- [Library of Congress](https://www.loc.gov/websites/collections/)
- [US National Archives](https://www.webharvest.gov/)
- [UK National Web Archives](http://www.nationalarchives.gov.uk/webarchive/)
- [UK Web Archive](https://www.webarchive.org.uk/)
- [Australian Web Archive (in TROVE)](https://webarchive.nla.gov.au/collection?q=) (successor of one of the earliest national web archives, [PANDORA](http://pandora.nla.gov.au/))

- [International Internet Preservation Consortium (IIPC)](https://netpreserve.org/) (organization supporting collaborative efforts to improve standards, tools, and best practices for web archiving)


[Memento](http://mementoweb.org/about/) "is a framework for accessing archived versions of web resources. Like many other web archiving services, Perma has implemented Memento. As a result, all public Perma Records are available via the Memento framework."
http://timetravel.mementoweb.org/about/


https://conifer.rhizome.org/
https://webrecorder.net/tools
https://replayweb.page/

wabac, https://wab.ac/
https://github.com/ikreymer/wabac.js

archivethumbs, https://github.com/machawk1/ArchiveThumbnails

WARCreate, https://warcreate.com/
WAIL, https://machawk1.github.io/wail/

Reconstructive, https://oduwsdl.github.io/Reconstructive/

Social Feed Manager, https://gwu-libraries.github.io/sfm-ui/

The landscape of tools *just* changed, so the main service folks use is now called Conifer (was Webrecorder.io), worth exploring,

Conifer (web based platform), https://conifer.rhizome.org/
Webrecorder Desktop (new desktop version), https://webrecorder.net/tools
ReplayWeb (tool to view the web archive files), https://replayweb.page/

With webrecorder you basically navigate around and it captures it as you surf, so any authentication is yours and it is as if from your computer (so proxy would work). The main idea was for social media capture. There is a lot of ethics to consider there, but certainly seems reasonable to capture public pages of organizations etc.

The other main method is to use wget, a commandline application. I use it to capture some university web properties on a regular basis. Here is an intro I wrote awhile ago, https://evanwill.github.io/_drafts/notes/wget-archives.html
